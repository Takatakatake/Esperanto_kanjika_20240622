{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc223e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##エスペラント文の文字形式の変換関数\n",
    "esperanto_to_x = { \"ĉ\": \"cx\", \"ĝ\": \"gx\", \"ĥ\": \"hx\", \"ĵ\": \"jx\", \"ŝ\": \"sx\", \"ŭ\": \"ux\",\n",
    "                   \"Ĉ\": \"Cx\", \"Ĝ\": \"Gx\", \"Ĥ\": \"Hx\", \"Ĵ\": \"Jx\", \"Ŝ\": \"Sx\", \"Ŭ\": \"Ux\",\n",
    "                   \"c^\": \"cx\", \"g^\": \"gx\", \"h^\": \"hx\", \"j^\": \"jx\", \"s^\": \"sx\", \"u^\": \"ux\",\n",
    "                    \"C^\": \"Cx\", \"G^\": \"Gx\", \"H^\": \"Hx\", \"J^\": \"Jx\", \"S^\": \"Sx\", \"U^\": \"Ux\"}\n",
    "x_to_jijofu={'cx': 'ĉ', 'gx': 'ĝ', 'hx': 'ĥ', 'jx': 'ĵ', 'sx': 'ŝ', 'ux': 'ŭ', 'Cx': 'Ĉ',\n",
    "             'Gx': 'Ĝ', 'Hx': 'Ĥ', 'Jx': 'Ĵ', 'Sx': 'Ŝ', 'Ux': 'Ŭ'}\n",
    "x_to_hat={'cx': 'c^', 'gx': 'g^', 'hx': 'h^', 'jx': 'j^', 'sx': 's^', 'ux': 'u^', 'Cx': 'C^',\n",
    "          'Gx': 'G^', 'Hx': 'H^', 'Jx': 'J^', 'Sx': 'S^', 'Ux': 'U^'}\n",
    "\n",
    "def replace_esperanto_chars(text,letter_dictionary):\n",
    "    for esperanto_char, x_char in letter_dictionary.items():\n",
    "        text = text.replace(esperanto_char, x_char)\n",
    "    return text\n",
    "#テスト用のエスペラント文\n",
    "# text = \"Ĝis revido! Mia nomo estas Ĵoĥano. Ĉu vi ŝatas ĥorojn? -Ne, mi s^tas felic^on. C^ S^ H^ c^ s^ h^  Ĉ Ĝ  Gxis revido! Mia nomo estas Jxohxano. Cxu vi sxatas hxorojn? -Ne, mi sxtas felicxon. Cx Sx Hx cx sx hx  Cx Gx\"\n",
    "#エスペラント文の文字形式の変換\n",
    "# replaced_text = replace_esperanto_chars(text,esperanto_to_x)\n",
    "# replaced_text =replace_esperanto_chars(text,x_to_jijofu)\n",
    "# replaced_text =replace_esperanto_chars(text,x_to_hat)\n",
    "# print(\"元のテキスト:\", text)\n",
    "# print(\"置換後のテキスト:\", replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "052b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##まず、\"世界语全部单词_大约44100个(原pejvo.txt).txt\"を小文字、X形式に変換する。 (変換コード上では小文字、X形式で統一。)\n",
    "with open(\"世界语全部单词_大约44100个(原pejvo.txt)_original2024620_utf8_2列目以降修正_2通りの分解修正.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    converted_text=replace_esperanto_chars(text,esperanto_to_x)#エスペラントの特殊文字を対応するx形式に変換。\n",
    "    converted_text=converted_text.lower()#小文字に変換。\n",
    "\n",
    "#結果を\"tmp.txt\"に書き出す。\n",
    "with open('tmp.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "166c5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def contains_digit(s):#対象の文字列sに数字となりうる文字列(数字)が含まれるかどうかを確認する関数\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "result=[]\n",
    "# \"tmp.txt\"(\"世界语全部单词_大约44100个(原pejvo.txt).txt\"を小文字、X形式に変換したもの)を開く。\n",
    "with open(\"tmp.txt\", 'r', encoding='utf-8') as file:\n",
    "    # \"tmp.txt\"の各行をループ。\n",
    "    for line in file:\n",
    "        # ':'が出てくるまでの部分を取り出す。\n",
    "        line=line.replace('-','/')##20240618追加\n",
    "        word = line.split(\":\")[0]\n",
    "        word = word.lstrip('/')##日本版,1列目だけ\n",
    "        # さらに取り出した部分を'-'、' '、','で分ける。\",\"もごくまれに存在する('tial')\n",
    "        parts = re.split('-| |,', word)\n",
    "        # 各部分をループし、単語の語尾の形式によって品詞分類しながら、その語尾をカットする。\n",
    "        for jj in range(len(parts)):\n",
    "            if jj>=0:\n",
    "                part=parts[jj]\n",
    "                if not (contains_digit(part) or len(part)<2):##2文字以上\n",
    "                    if \"/\" in part:\n",
    "                        if part.endswith(('/o','/on','oj','/o!','ojn','on!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('名詞')\n",
    "                        elif part.endswith(('/a','/aj','/an','/an!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('形容詞')\n",
    "                        elif part.endswith(('/e','/e!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('副詞')\n",
    "                        elif part.endswith(('/e/n','/e/n!')):##'/e/n'は後で気をつける\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-2])]\n",
    "                            AA.append('副詞')    \n",
    "                        elif part.endswith(('/i','/u','/u!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('動詞')\n",
    "                        elif part.endswith(('/n')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('n語')            \n",
    "                    else:\n",
    "                        AA=[part,\"無詞\"]\n",
    "                    result.append(AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e262e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "##すべての単語の語尾が正しくカットされているかどうかチェックする。 ここまで情報は何一つ減っていないはず。\n",
    "with open(\"检查世界语所有单词的结尾是否被正确切除(result)_全て＿一時修正済み.txt\",\"w\",encoding='utf-8') as g:\n",
    "    for hh in result:\n",
    "        if len(hh)==2:##不要な条件と思われる。\n",
    "            g.write(hh[0]+','+hh[1]+\"\\n\")\n",
    "            # g.write(replace_esperanto_chars(hh[0],x_to_jijofu)+\"##\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c81690c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11222"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##上の作業で抽出した、'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リストを作成していく。\n",
    "##\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"こそが最終的な漢字置換リストの大元になる。\n",
    "##'既に完全に語根分解された状態の単語'が対象であれば、文字数の多い語根順に漢字置換するだけで完璧な精度の漢字置換ができる!\n",
    "##ただし、その完璧な漢字置換のためにはあらかじめ\"世界语全部单词_大约44100个(原pejvo.txt).txt\"から\"从世界语全部单词_大约44100个(原pejvo.txt).txt中提取并输出世界语所有词根_大约11360个.txt_带中文日文注释.ipynb\"を用いてエスペラントの全語根を抽出しておく必要がある。\n",
    "\n",
    "replacements_dict={}##一旦辞書型を使う。(後で内容(value)を更新するため)\n",
    "with open(\"世界语所有词根_大约11222个_20240621.txt\", 'r', encoding='utf-8') as file:\n",
    "# with open(\"世界语所有词根_大约11289个_20240619.txt\", 'r', encoding='utf-8') as file:\n",
    "    ##\"世界语所有词根_大约11360个.txt\"は\"世界语全部单词_大约44100个(原pejvo.txt).txt\"から\"从世界语全部单词_大约44100个(原pejvo.txt).txt中提取并输出世界语所有词根_大约11360个.txt_带中文日文注释.ipynb\"を用いて抽出したエスペラントの全語根である。\n",
    "    roots = file.readlines()\n",
    "    for root in roots:\n",
    "        root = root.strip()\n",
    "        if not root.isdigit():##混入していた数字の'10'と'7'を削除\n",
    "            replacements_dict[root]=[root,len(root)]##各エスペラント語根に対する'置換後の単語'(この作業では元の置換対象の語根のまま)と、その置換順序として、'置換対象の語根の文字数'を設定。　置換順序の数字が大きい('置換対象の語根の文字数が多い')ほど、先に置換される仕組みにする。\n",
    "len(replacements_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1ff9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "##上の作業に引き続き、\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リスト\"を作成していく。　\n",
    "##ここでは漢字置換リストに自分で作成したエスペラント語根の漢字化リストを反映させる。\n",
    "\n",
    "def conversion_format(hanzi,word):##変換形式を決める。\n",
    "    return '<ruby>{}<rt>{}</rt></ruby>'.format(hanzi,word)\n",
    "\n",
    "input_file=\"20240316世界语词根列表＿包含2个字符的世界语词根.csv\"\n",
    "# input_file=\"世界语汉字表格_20240312.csv\"\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        j = line.split(',')\n",
    "        if len(j)>=2:\n",
    "            word,hanzi=j[0],j[1]\n",
    "            if (hanzi!='') and (word!='') and ('#' not in word):\n",
    "                replacements_dict[word]=[conversion_format(hanzi,word),len(word)]#辞書式配列では要素(key)に対する値(value)を後から更新できることを利用している。\n",
    "# with open(\"replacements_dict.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for old,new in replacements_dict.items():\n",
    "#         file.write(f'{old},{new[0]},{new[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80537419",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'漢字置換リスト'を置換順序の数字の大きさ順にソート。\n",
    "pre_replacements=[]\n",
    "for old,new in replacements_dict.items():\n",
    "    pre_replacements.append((old,new[0],new[1]))\n",
    "pre_replacements3 = sorted(pre_replacements, key=lambda x: x[2], reverse=True)\n",
    "# pre_replacements3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f4ce9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##'漢字置換リスト'の各置換に対して'place holder'を追加し、'replacements'リストとして完成させた。\n",
    "##'place holder'法とは、既に置換を終えた文字列が後続の置換によって重複して置換されてしまうことを避けるために、その置換を終えた部分に一時的に無関係な文字列(place holder)を置いておいて、\n",
    "##全ての置換が終わった後に、改めてその'無関係な文字列(place holder)'から'目的の置換後文字列'に変換していく手法である。\n",
    "\n",
    "with open('No.10000_500000.txt', 'r', encoding='utf-8') as file:##漢字置換時に用いる\"place holder\"ファイルを予め読み込んでおく。\n",
    "    loaded_strings = [line.strip() for line in file]\n",
    "\n",
    "replacements=[]\n",
    "for kk in range(len(pre_replacements3)):\n",
    "    replacements.append([pre_replacements3[kk][0],pre_replacements3[kk][1],loaded_strings[kk]])\n",
    "\n",
    "\n",
    "##置換に用いる関数。正規表現、C++など様々な形式の置換を試したが、pythonで'place holder'を用いる形式の置換が、最も処理が高速であった。(しかも大変シンプルでわかりやすい。)\n",
    "def safe_replace(text, replacements):\n",
    "    valid_replacements = {}\n",
    "    # 置換対象(old)を'place holder'に一時的に置換\n",
    "    for old, new, placeholder in replacements:\n",
    "        if old in text:\n",
    "            text = text.replace(old, placeholder)\n",
    "            valid_replacements[placeholder] = new# 後で置換後の文字列(new)に置換し直す必要がある'place holder'を辞書(valid_replacements)に記録しておく。\n",
    "    #'place holder'を置換後の文字列(new)に置換)\n",
    "    for placeholder, new in valid_replacements.items():\n",
    "        text = text.replace(placeholder, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#replacementsリスト(\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リスト\"の完成版)の内容確認\n",
    "with open(\"replacements_list.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,priority in replacements:\n",
    "        file.write(f'{old},{new},{priority}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2b206c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50674"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84a1e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を実際にreplacementsリスト(漢字置換リストの完成版)によって漢字置換。　\n",
    "##ここで作成される、\"漢字置換し終えたリスト(辞書型)\"(SS)が最終的な漢字置換リストの大元になる。\n",
    "##リストresultまでは情報の損失は殆どないはず。\n",
    "\n",
    "SS={}\n",
    "for j in result:##20秒ほどかかる。　先にリストの要素を全て結合して、一つの文字列にしてから漢字置換する方法を試しても(上述)、さほど高速化しなかった。\n",
    "    if len(j)==2:##(j[0]がエスペラント語根、j[1]が品詞。)\n",
    "        if len(j[0])>=2:##2文字以上のエスペラント語根のみが対象\n",
    "            if j[0] in SS:\n",
    "                if j[1] not in SS[j[0]][1]:\n",
    "                    SS[j[0]] = [SS[j[0]][0],SS[j[0]][1] + ', ' + j[1]]##複数品詞の追加\n",
    "            else:\n",
    "                SS[j[0]]=[safe_replace(j[0], replacements),j[1]]##辞書敷配列の追加法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0c0c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abak: ['abak', '名詞']\n",
      "abat/ec: ['abat/<ruby>性<rt>ec</rt></ruby>', '名詞']\n",
      "abat/ej: ['abat/<ruby>场<rt>ej</rt></ruby>', '名詞']\n",
      "abat/in: ['abat/<ruby>女<rt>in</rt></ruby>', '名詞']\n",
      "abat: ['abat', '名詞']\n",
      "abdik: ['<ruby>退<rt>abdik</rt></ruby>', '動詞, 名詞']\n",
      "abdomen: ['abdomen', '名詞']\n",
      "abel: ['<ruby>蜜蜂<rt>abel</rt></ruby>', '形容詞, 名詞']\n",
      "miel: ['<ruby>蜜<rt>miel</rt></ruby>', '名詞, 形容詞']\n",
      "pik/il: ['<ruby>刺<rt>pik</rt></ruby>/<ruby>具<rt>il</rt></ruby>', '名詞']\n"
     ]
    }
   ],
   "source": [
    "# # テキストファイルから辞書を再構築する\n",
    "# reconstructed_dict = {}\n",
    "# with open(\"SS.txt\", 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         # 各行をコンマで分割\n",
    "#         parts = line.strip().split(',')\n",
    "#         if len(parts)>=3:\n",
    "#             # 分割したデータを辞書に追加（リストとして保存）\n",
    "#             reconstructed_dict[parts[0]] = [parts[1], parts[2]]\n",
    "# # 再構築された辞書の内容を表示\n",
    "# for key, value in dict(list(reconstructed_dict.items())[:5]).items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "# 上の作業で作成した辞書型リスト(SS)の最初から20個分を表示\n",
    "for key, value in dict(list(SS.items())[:10]).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "968bc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここから、\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト(辞書型)\"(SS)を最終的な漢字置換リストに成形していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1bbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##更改替换方式(关于如何更改汉字转换,请编辑第一个csv文件)  (置換の仕方の変更(漢字変換の仕方の変更については最初のcsvファイルを編集する))\n",
    "# never_used_as_roots_only=[\" vin \",\" lin \",\" min \",\" amas \"]\n",
    "# for i in never_used_as_roots_only:\n",
    "#     SS[i]=[i,\"無詞\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cc487f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここで、2通りの語根分解を消去していると考えられるが、品詞別に語根分解の方法を残すべきではないのか→日本語版の全単語語根分解リストについては解決。\n",
    "\n",
    "## SS→QQ  (\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"(SS)を最終的な漢字置換リストに成形していく。)\n",
    "##SSの'置換対象の単語'、'漢字置換後の単語'から\"/\"を抜く(html形式にしたい場合、\"</rt></ruby>\"は\"/\"を含むので要注意！)。\n",
    "##新たに置換優先順位を表す数字を追加し(漢字化する単語は'文字数×10000'、漢字化しない単語は'文字数×10000-2500')、辞書式配列QQとして保存。\n",
    "QQ={}\n",
    "for i,j in SS.items():##(iが置換対象の単語、j[0]が漢字置換後の単語、j[1]が品詞。)\n",
    "    if i==j[0]:##漢字化しない単語\n",
    "        QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000-2500]##漢字化しない単語は優先順位を下げる\n",
    "    else:\n",
    "        QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000]\n",
    "\n",
    "# QQ\n",
    "with open(\"QQ.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  QQ.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]},{new[2]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73637b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 基本的には動詞に対してのみ活用語尾を追加し、置換対象の単語の文字数を増やす(置換の優先順位を上げる。)\n",
    "noun_prefix_2l={}\n",
    "noun_suffix_2l={}\n",
    "adj_prefix_2l={}\n",
    "adj_suffix_2l={}\n",
    "adv_prefix_2l={}\n",
    "adv_suffix_2l={}\n",
    "verb_prefix_2l={}\n",
    "verb_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us','at':'at','it':'it','ot':'ot', 'ad':'ad','igx':'igx','ig':'ig','ant':'ant','int':'int','ont':'ont'}\n",
    "##接頭辞接尾時の追加については、主に動詞が対象である。\n",
    "\n",
    "noun_prefix_2l_2={}\n",
    "for d1,d2 in noun_prefix_2l.items():\n",
    "    noun_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "noun_suffix_2l_2={}\n",
    "for d1,d2 in noun_suffix_2l.items():\n",
    "    noun_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "adj_prefix_2l_2={}\n",
    "for d1,d2 in adj_prefix_2l.items():\n",
    "    adj_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "adj_suffix_2l_2={}\n",
    "for d1,d2 in adj_suffix_2l.items():\n",
    "    adj_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "adv_prefix_2l_2={}\n",
    "for d1,d2 in adv_prefix_2l.items():\n",
    "    adv_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "adv_suffix_2l_2={}\n",
    "for d1,d2 in adv_suffix_2l.items():\n",
    "    adv_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "verb_prefix_2l_2={}\n",
    "for d1,d2 in verb_prefix_2l.items():\n",
    "    verb_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "verb_suffix_2l_2={}\n",
    "for d1,d2 in verb_suffix_2l.items():\n",
    "    verb_suffix_2l_2[d1]=safe_replace(d2, replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea351ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###一番の工夫ポイント(如何にして置換の優先順位を定め、置換精度を向上させるか。)\n",
    "##基本は単語の文字数が多い順に置換していくことになるが、\n",
    "##例えば、\"置換対象の単語に接頭辞、接尾辞を追加し、単語の文字数を増やし、置換の優先順位を上げたものを、置換対象の単語として新たに追加する。\"などが、置換精度を上げる方策として考えられる。\n",
    "##しかし、いろいろ試した結果、動詞に対してのみ活用語尾を追加し、置換対象の単語の文字数を増やす(置換の優先順位を上げる。)のが、ベストに近いことがわかった。\n",
    "\n",
    "## SS→QQ→RR  (\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"(SS)を最終的な漢字置換リストに成形していく。)\n",
    "RR={}\n",
    "# 辞書をコピーする\n",
    "QQ_copy = QQ.copy()\n",
    "for i,j in QQ_copy.items():##j[0]:置換後の文字列　j[1]:品詞 j[2]:置換優先順位\n",
    "    if (j[1] == \"名詞\") and (len(i)<=6) and not(j[2]==60000 or j[2]==50000 or j[2]==40000 or j[2]==30000 or j[2]==20000):##名詞だけで、6文字以下で、漢字化しないやつ  ##置換ミスを防ぐための条件(20240614) altajo 固有名詞対策  意味ふりがなのときは再検討\n",
    "        for k1,k2 in noun_prefix_2l.items():\n",
    "            if not k1+i in QQ_copy:\n",
    "                RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]#既存でないものは優先順位を大きく下げる\n",
    "        for k1,k2 in noun_suffix_2l.items():##\"obl\",\"on\",\"op\"現在はなし\n",
    "            if not i+k1 in QQ_copy:\n",
    "                RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]#既存でないものは優先順位を大きく下げる\n",
    "        for k in [\"o\"]:\n",
    "            if not i+k in QQ_copy:\n",
    "                RR[i+k]=[j[0]+k,j[2]+1*10000-5000]#既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "        QQ.pop(i, None)\n",
    "\n",
    "for i,j in QQ.items():##j[0]:置換後の文字列　j[1]:品詞 j[2]:置換優先順位\n",
    "    # if len(i)<=2 and i==j[0]:##2文字以下の語根で漢字化されないものは削除 もしくは2文字以下の語根すべてを削除するのも有りかもしれない。\n",
    "    if len(i)<=2:##1文字は存在しないはずではある。\n",
    "        ##基本的に非動詞の2文字の語根単体を以て漢字置換することはない。　ただし、世界语全部单词_大约44100个(原pejvo.txt).txtに最初から含まれている2文字の語根は既に漢字化されており、実際の漢字置換にも反映されることになる。\n",
    "        ##2文字の語根でも、動詞については活用語尾を追加することで、自動的に+2文字以上できるので追加した。\n",
    "        if \"名詞\" in j[1]:\n",
    "            for k in [\"o\",\"on\",'oj','ojn']:\n",
    "                if not i+k in QQ:\n",
    "                    RR[' '+i+k+' ']=[' '+j[0]+k+' ',j[2]+len(k)*10000-2000]\n",
    "        if \"形容詞\" in j[1]:\n",
    "            for k in [\"a\",\"aj\",\"ajn\",'an']:\n",
    "                if not i+k in QQ:\n",
    "                    RR[' '+i+k+' ']=[' '+j[0]+k+' ',j[2]+len(k)*10000-2000]\n",
    "        if \"副詞\" in j[1]:\n",
    "            for k in [\"e\",'en']:\n",
    "                if not i+k in QQ:\n",
    "                    RR[' '+i+k+' ']=[' '+j[0]+k+' ',j[2]+len(k)*10000-2000]\n",
    "        if \"動詞\" in j[1]:\n",
    "            for k1,k2 in verb_prefix_2l.items():\n",
    "                if not k1+i in QQ:\n",
    "                    RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]\n",
    "            for k1,k2 in verb_suffix_2l.items():\n",
    "                if not i+k1 in QQ:\n",
    "                    RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]\n",
    "            for k in [\"u \",\"u!\",\"i \"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                if not i+k in QQ:\n",
    "                    RR[i+k]=[j[0]+k,j[2]+2*10000-5000]\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        RR[i]=[j[0],j[2]]##品詞情報はここで用いるためにあった。以後は不要なので省いていく。\n",
    "        if \"名詞\" in j[1] and (len(i)<=6) :##名詞については形容詞、副詞と違い、漢字化しないものにもoをつける。\n",
    "            for k1,k2 in noun_prefix_2l.items():\n",
    "                if not k1+i in QQ:\n",
    "                    RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]#既存でないものは優先順位を大きく下げる\n",
    "            for k1,k2 in noun_suffix_2l.items():##\"obl\",\"on\",\"op\",\n",
    "                if not i+k1 in QQ:\n",
    "                    RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]#既存でないものは優先順位を大きく下げる\n",
    "            for k in [\"o\"]:\n",
    "                if not i+k in QQ:\n",
    "                    RR[i+k]=[j[0]+k,j[2]+1*10000-5000]#既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "        if j[2]==50000 or j[2]==40000 or j[2]==30000 or j[2]==20000:##文字数が比較的少なく(<=5)、実際に漢字化するエスペラント語根(文字数×10000)のみを対象とする \n",
    "            if \"形容詞\" in j[1]:\n",
    "                for k1,k2 in adj_prefix_2l.items():\n",
    "                    if not k1+i in QQ:\n",
    "                        RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]\n",
    "                for k1,k2 in adj_suffix_2l.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]\n",
    "                for k in [\"a\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+1*10000-5000] \n",
    "            if \"副詞\" in j[1]:\n",
    "                for k1,k2 in adv_prefix_2l.items():\n",
    "                    if not k1+i in QQ:\n",
    "                        RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]\n",
    "                for k1,k2 in adv_suffix_2l.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]\n",
    "                for k in [\"e\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+1*10000-5000]  \n",
    "            if \"動詞\" in j[1]:\n",
    "                for k1,k2 in verb_prefix_2l.items():\n",
    "                    if not k1+i in QQ:\n",
    "                        RR[k1+i]=[k2+j[0],j[2]+2*10000-5000]\n",
    "                for k1,k2 in verb_suffix_2l.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+2*10000-5000]\n",
    "                for k in [\"u \",\"u!\",\"i \"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+2*10000-5000]\n",
    "        # if (j[1] == \"名詞\") and (len(i)<=6) and not(j[2]==60000 or j[2]==50000 or j[2]==40000 or j[2]==30000 or j[2]==20000):##名詞だけで、6文字以下で、漢字化しないやつ  ##置換ミスを防ぐための条件(20240614) altajo 固有名詞対策  意味ふりがなのときは再検討\n",
    "        #     RR.pop(i, None)  最初に消すべきと判断\n",
    "                \n",
    "##in QQで良かったっけ？ →　辞書ではキーの完全一致が必要なため、部分的な一致では True を得ることはできないので大丈夫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca9462ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zumkant'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i##zurikが消えている点に注目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d0a48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RR.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  RR.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42b3606c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datumi': ['<ruby>日<rt>dat</rt></ruby>umi', 62500],\n",
       " 'trameti': ['<ruby>通<rt>tra</rt></ruby><ruby>置<rt>met</rt></ruby>i', 72500]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b='dat/um/i'\n",
    "a=safe_replace(b,replacements)\n",
    "PP={}\n",
    "# PP[b.replace('/', '')]=[a.replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),len(b)*10000+2500]\n",
    "PP['dat/um/i'.replace('/', '')]=[safe_replace('dat/um/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/i'.replace('/', ''))*10000+2500]\n",
    "PP['tra/met/i'.replace('/', '')]=[safe_replace('tra/met/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/i'.replace('/', ''))*10000+2500]\n",
    "PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9837296",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RRの編集(主に置換の優先順位の変更) ここでも置換の仕方の変更ができないことはないが、品詞の種類に応じて接尾辞や接頭辞を追加するところをスキップすることになってしまう。\n",
    "# def conversion_format(hanzi,word):##変換形式を決める。\n",
    "#     return '<ruby>{}<rt>{}</rt></ruby>'.format(hanzi,word)\n",
    "\n",
    "never_used_as_roots_only=[\"vin\",\"lin\",\"sin\",\"min\"]\n",
    "for i in never_used_as_roots_only:\n",
    "    RR[i]=[i,len(i)*10000+2500]##これらについては数字の大きさはそこまで重要ではない\n",
    "# QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000-2500]##漢字化しない単語は優先順位を下げる\n",
    "# conversion_format(hanzi,word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RR['amas']=['<ruby>爱<rt>am</rt></ruby>as',len('amas')*10000+2500]##漢字化しない語根単体については上記で、うまく処理できているはずだが、amasoは群oと漢字化するので。\n",
    "RR['farigx'][1]=len('farigx')*10000+27500##優先順位だけ変更\n",
    "\n",
    "x='mondo/n'\n",
    "RR[x.replace('/', '')]=[safe_replace(x,replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len(x.replace('/', ''))*10000+2500]\n",
    "\n",
    "\n",
    "##以下は完全手作業\n",
    "RR['dat/um/i'.replace('/', '')]=[safe_replace('dat/um/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/i'.replace('/', ''))*10000+2500]\n",
    "RR['dat/um/u'.replace('/', '')]=[safe_replace('dat/um/u',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/u'.replace('/', ''))*10000+2500]\n",
    "RR['dat/um/u!'.replace('/', '')]=[safe_replace('dat/um/u!',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/u!'.replace('/', ''))*10000+2500]\n",
    "#dat/um/u  dat/um/u!\n",
    "RR['tra/met/i'.replace('/', '')]=[safe_replace('tra/met/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/i'.replace('/', ''))*10000+2500]\n",
    "RR['tra/met/u'.replace('/', '')]=[safe_replace('tra/met/u',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/u'.replace('/', ''))*10000+2500]\n",
    "RR['tra/met/u!'.replace('/', '')]=[safe_replace('tra/met/u!',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/u!'.replace('/', ''))*10000+2500]\n",
    "\n",
    "# RR['trametu!']=['<ruby>通<rt>tra</rt></ruby><ruby>置<rt>met</rt></ruby>u!',len('trametu!')*10000+2500]\n",
    "#tra/met/u  tra/met/u!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08c5cf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ruby>矿<rt>min</rt></ruby>'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_replace(\"amas\",replacements)##リストreplacementはコードの前半と後半で別物になるので注意が必要。\n",
    "safe_replace(\"min\",replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4011280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TT=[]\n",
    "for old,new in  RR.items():\n",
    "    if isinstance(new[1], int):\n",
    "        TT.append((old,new[0],new[1]))\n",
    "# print(len(TT),len(RR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "053fc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_replacements3= sorted(TT, key=lambda x: x[2], reverse=True)##(置換順序の数字の大きさ順にソート!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3ec2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pre_replacements3.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,priority in pre_replacements3:\n",
    "        file.write(f'{old},{new},{priority}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "adcb681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'エスペラント語根'、'置換漢字'、'place holder'の順に並べ、最終的な置換に用いる\"replacements\"リストを作成。\n",
    "replacements=[]\n",
    "for kk in range(len(pre_replacements3)):\n",
    "    if len(pre_replacements3[kk][0])>1:\n",
    "        replacements.append([pre_replacements3[kk][0],pre_replacements3[kk][1],loaded_strings[kk]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8511730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56266"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e749f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168798"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##'大文字'、'小文字'、'文頭だけ大文字'のケースに対応。\n",
    "replacements2=[]\n",
    "for old,new,place_holder in replacements:\n",
    "    replacements2.append((old,new,place_holder))\n",
    "    replacements2.append((old.upper(),new.upper(),place_holder[:-1]+'up$'))##place holderを少し変更する必要があった。\n",
    "    if old[0]==' ':\n",
    "        replacements2.append((old[0] + old[1].upper() + old[2:],new[0] + new[1].upper() + new[2:],place_holder[:-1]+'cap$'))##new[0] + new[1].upper() + new[2:]は本当は怪しいが。。  \n",
    "    else:\n",
    "        replacements2.append((old.capitalize(),new.capitalize(),place_holder[:-1]+'cap$'))\n",
    "len(replacements2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b74ca0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"replacements2\"リストの内容を確認\n",
    "with open(\"replacements2_list_html.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,place_holder in replacements2:\n",
    "        file.write(f'{old},{new},{place_holder}\\n')\n",
    "#最終的な置換に用いる\"replacements2\"リストの長さ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af099f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements3=[]\n",
    "with open(\"replacements2_list_html.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()##stripでは置換に最初の空白を反映できなくなってしまう。\n",
    "        j = line.split(',')\n",
    "        if len(j)==3:\n",
    "            old,new,place_holder=j[0],j[1],j[2]\n",
    "            replacements3.append((old,new,place_holder))\n",
    "#len(replacements3)  \n",
    "#replacements2とreplacements3は全く同一のリストである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7042a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##error check\n",
    "# replacements3=[]\n",
    "# with open(\"replacements2_list.txt\", 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         line = line.strip()\n",
    "#         j = line.split(',')\n",
    "#         if len(j)==5:##3でない場合を試す。\n",
    "#             old,new,place_holder,unknown1,unknown2=j[0],j[1],j[2],j[3],j[4]\n",
    "#             replacements3.append((old,new,place_holder,unknown1,unknown2))\n",
    "# replacements3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82324670",
   "metadata": {},
   "outputs": [],
   "source": [
    "##置換に用いる関数。正規表現、C++など様々な形式の置換を試したが、pythonで'place holder'を用いる形式の置換が、最も処理が高速であった。(しかも大変シンプルでわかりやすい。)\n",
    "def safe_replace(text, replacements):\n",
    "    valid_replacements = {}\n",
    "    # 置換対象(old)を'place holder'に一時的に置換\n",
    "    for old, new, placeholder in replacements:\n",
    "        if old in text:\n",
    "            text = text.replace(old, placeholder)\n",
    "            valid_replacements[placeholder] = new# 後で置換後の文字列(new)に置換し直す必要がある'place holder'を辞書(valid_replacements)に記録しておく。\n",
    "    #'place holder'を置換後の文字列(new)に置換)\n",
    "    for placeholder, new in valid_replacements.items():\n",
    "        text = text.replace(placeholder, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "#テストデータ\n",
    "text = \"\"\"Post-diplomaj studoj pri inter-lingvistiko: pinto kaj fino\n",
    "\n",
    "Interlingvistiko estas la studo de internacia komunikado kaj homaj rilatoj interalie pere de plan-lingvoj kiel Esperanto.\n",
    "\n",
    "La tri-jaraj post-diplomaj Interlingvistikaj Studoj (IS) – kiuj funkcias ekde 1998 en la pola Universitato Adam Mickiewicz (UAM) en Poznano – komencis sian lastan studjaron en septembro 2023. La internacia kursaro okazas sub la gvidado de prof. d-ro Ilona Koutny, kun la kontribuo de fame konataj esperantologoj el pluraj landoj. \n",
    "\n",
    "La tria jaro de la studoj ebligas specialiĝojn. Ĉi-foje eblis elekti inter Tradukado, Interlingvistiko kaj Esperanto-movado, aŭ Instruista trejnado. \n",
    "\n",
    "Krom la specialiĝoj, la studentoj daŭre partoprenas komunajn kursojn kiel grupo. Inter tiuj kursoj estis la stud-objektoj Nuntempa Esperanto-literaturo kaj Nuntempaj problemoj de la Esperanto-movado. Tiun lastan gvidis Mark Fettes, kiu estis prezidanto de Universala Esperanto-Asocio (UEA) de 2013 ĝis 2019. Surbaze de siaj propraj spertoj, li gvidis la studentojn en interesaj diskutoj.\n",
    "\n",
    "UAM alte taksas, ke la Interlingvistikaj Studoj kontribuas al la internaciigo de la universitato. Efektive, krom el Pollando, la studentoj devenas de 11 landoj: Brazilo, Britio, Ĉeĥio, Ĉinio, Francio, Indonezio, Italio, Koreio, Kroatio, Rumanio, Vjetnamio. \n",
    "\n",
    "En tiu ĉi jaro la studentoj verkas ankaŭ diplom-laboraĵon. Siajn rezultojn ili prezentos en septembro 2024. La plej bonaj laboraĵoj estas regule eldonataj, ekzemple en la lingvistika revuo Investigationes Lingvisticae de UAM.\n",
    "\n",
    "Post 25 jaroj, tiu ĉi estas la lasta tri-jara periodo de la studoj. Kvazaŭ por festi la 25-jariĝon, en la pasinta studjaro komenciĝis la magistraj studoj pri interlingvistiko, kadre de la studfako Lingvistiko kaj inform-mastrumado. La studoj bone progresas kun dek studentoj, el kiuj sep venas el la ĉina Universitato en Zaozhuang, kie funkcias Esperanto-fako.\n",
    "\n",
    "En 2024 la studentoj de ambaŭ programoj finos siajn studojn, kaj tiel finiĝos la historio de interlingvistiko en UAM. Kiel lasta pinta evento okazos labor-sesio pri interlingvistiko kadre de la fama internacia Lingvista Kongreso, kiu ĉi-jare okazos en Poznano, en septembro 2024.\n",
    "\n",
    "Roksana estis elstara gastigantino kaj iun vesperon ŝi rakontis al mi la sekvan rakonton.  Samoso estis princ-lando interne de la Turka Imperio. Iam la sultano en Istanbulo havis fortan doloron ĉe la oreloj, kiun la istanbulaj kuracistoj ne sukcesis kuraci. Iu informis la sultanon, ke en la princlando Samoso estas fama kuracisto. La sultano sendis ŝipon por alvenigi la kuraciston Stamatiadis, kun la averto, ke li estos kruele punita, se io misfunkcios. Sed post unu semajno la oreloj de la sultano ne plu doloris. Kiel en fabeloj la sultano diris al Stamatiadis: “Dankon pro la kuracado. Petu ion ajn, kaj mi donos tion al vi.” Stamatiadis diris: “Mi volas, ke en la princlando Samoso oni instruu Esperanton en ĉiuj lernejoj.” La sultano turnis sin al iu ministro kaj diris: \"Tiel estu!\"\n",
    "\n",
    "La ministro pretigis leteron de la sultano al la princo de Samoso kun la ordono, ke oni instruu Esperanton en la lernejoj, kaj la ŝipo reportis Stamatiadis kaj la leteron al Samoso. La princo ne havis elekton: li nepre devis ordoni al la lernejoj (tiam estis eble du-tri en tiu insulo) instrui Esperanton. Sed la pastroj kaj la publiko estis tre malkontentaj, ĉar Stamatiadis estis konata socialisto, kaj oni konsideris lin malamiko de Dio kaj de la kristana religio. La pastroj predikis al la popolo, ke oni devas malhelpi tiel teruran aferon pro kulpo de la malforta princo, kiu obeis ordonon de la islama imperiestro instrui lingvon de la diablo. Ili tiel efike predikis, ke la popolo bruligis la lernejojn. Tiam la princo ĉesigis la instruadon de Esperanto, kaj Stamatiadis mem transloĝiĝis al Istanbulo kaj poste al Ateno. La tuta afero okazis ĉirkaŭ la jaro 1910.\n",
    "\n",
    "Tiu estas la rakonto, kiun mi aŭdis de Roksana Stamatiadis, (aŭ \"s-ino Manusu\" laŭ la nomo de ŝia edzo). Naskiĝinte en 1900, ŝi poste mortis en 1981.\n",
    "\n",
    "Sed laŭ esploroj el aliaj fontoj la realo estis iom malsama kaj malpli romantika. Stamatiadis estis grava kuracisto en Samoso kaj aktiva esperantisto. Li fondis Esperanto-grupon en Samoso ĉirkaŭ la jaro 1905, eldonis gazeton, kaj konvinkis la princon fari dekreton pri lernado de Esperanto en la lernejoj de Samoso. La lernado de Esperanto ĉesis kiam la princo estis mortigita kaj Stamatiadis estis ekzilita al Istanbulo pro neklaraj politikaj kialoj. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f3697d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multiprocessingのための関数群　テキストを行数によって設定プロセス数(num_processes)に等分割して、それぞれのプロセスで並列に置換処理を実行してから、再度分割したテキストを結合する。\n",
    "\n",
    "import multiprocessing\n",
    "def process_segment(lines, replacements):\n",
    "    # 文字列のリストを結合してから置換処理を実行 linesには\\nが含まれていない状態の文字列群が格納されている。\n",
    "    segment = '\\n'.join(lines)\n",
    "    segment = safe_replace(segment, replacements)#ここでsafe_replace関数の実行\n",
    "    return segment\n",
    "\n",
    "def parallel_process(text, num_processes,replacements):\n",
    "    #テキストを行で分割\n",
    "    lines = text.split('\\n')\n",
    "    num_lines = len(lines)\n",
    "    lines_per_process = num_lines // num_processes\n",
    "\n",
    "    #各プロセスに割り当てる行のリストを決定\n",
    "    ranges = [(i * lines_per_process, (i + 1) * lines_per_process) for i in range(num_processes)]\n",
    "    ranges[-1] = (ranges[-1][0], num_lines)  #最後のプロセスが残り全てを処理\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # 並列処理を実行\n",
    "        results = pool.starmap(process_segment, [(lines[start:end], replacements) for start, end in ranges])\n",
    "\n",
    "    # 結果を結合\n",
    "    return '\\n'.join(result for result in results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "482dfe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ruby>后<rt>post</rt></ruby>-diplomaj <ruby>学<rt>stud</rt></ruby>oj <ruby>关<rt>pri</rt></ruby> <ruby>间<rt>inter</rt></ruby>-lingvistiko: <ruby>尖<rt>pint</rt></ruby>o kaj <ruby>结束<rt>fin</rt></ruby>o\\n\\nInterlingvistiko <ruby>是<rt>est</rt></ruby>as la <ruby>学<rt>stud</rt></ruby>o de <ruby>间<rt>inter</rt></ruby><ruby>民<rt>naci</rt></ruby>a <ruby>沟通<rt>komunik</rt></ruby>ado kaj <ruby>人<rt>hom</rt></ruby>aj <ruby>关系<rt>rilat</rt></ruby>oj <ruby>间<rt>inter</rt></ruby><ruby>异<rt>ali</rt></ruby>e <ruby>毁<rt>pere</rt></ruby> de <ruby>计划<rt>plan</rt></ruby>-<ruby>语<rt>lingv</rt></ruby>oj <ruby>何方<rt>kiel</rt></ruby> <ruby>语<rt>esperant</rt></ruby>o.\\n\\nLa tri-<ruby>年<rt>jar</rt></ruby>aj <ruby>后<rt>post</rt></ruby>-diplomaj Interlingvistikaj <ruby>学<rt>stud</rt></ruby>oj (IS) – <ruby>何u<rt>kiu</rt></ruby>j <ruby>功能<rt>funkci</rt></ruby>as ekde 1998 en la pola <ruby>大学<rt>universitat</rt></ruby>o Adam Mic<ruby>何e<rt>kie</rt></ruby>wicz (UAM) en <ruby>姿<rt>poz</rt></ruby>nano – <ruby>开始<rt>komenc</rt></ruby>is <ruby>自<rt>si</rt></ruby>an <ruby>末<rt>last</rt></ruby>an <ruby>学<rt>stud</rt></ruby><ruby>年<rt>jar</rt></ruby>on en <ruby>九月<rt>septembr</rt></ruby>o 2023. La <ruby>间<rt>inter</rt></ruby><ruby>民<rt>naci</rt></ruby>a <ruby>课<rt>kurs</rt></ruby>aro <ruby>发生<rt>okaz</rt></ruby>as <ruby>下<rt>sub</rt></ruby> la <ruby>导<rt>gvid</rt></ruby>ado de prof. d-ro Ilona Koutny, <ruby>共<rt>kun</rt></ruby> la <ruby>贡献<rt>kontribu</rt></ruby>o de <ruby>名<rt>fam</rt></ruby>e <ruby>识<rt>kon</rt></ruby>ataj esperantologoj el <ruby>多<rt>plur</rt></ruby>aj <ruby>土<rt>land</rt></ruby>oj. \\n\\nLa tria <ruby>年<rt>jar</rt></ruby>o de la <ruby>学<rt>stud</rt></ruby>oj <ruby>能<rt>ebl</rt></ruby><ruby>使<rt>ig</rt></ruby>as <ruby>特<rt>special</rt></ruby><ruby>使<rt>ig</rt></ruby>xojn. Cxi-<ruby>次<rt>foj</rt></ruby>e <ruby>能<rt>ebl</rt></ruby>is <ruby>选择<rt>elekt</rt></ruby>i <ruby>间<rt>inter</rt></ruby> <ruby>翻译<rt>traduk</rt></ruby>ado, Interlingvistiko kaj <ruby>语<rt>esperant</rt></ruby>o-<ruby>动<rt>mov</rt></ruby>ado, aux <ruby>教<rt>instru</rt></ruby><ruby>家<rt>ist</rt></ruby>a <ruby>训练<rt>trejn</rt></ruby>ado. \\n\\n<ruby>外<rt>krom</rt></ruby> la <ruby>特<rt>special</rt></ruby><ruby>使<rt>ig</rt></ruby>xoj, la <ruby>学生<rt>student</rt></ruby>oj <ruby>持<rt>dauxr</rt></ruby>e <ruby>部分<rt>part</rt></ruby>o<ruby>拿<rt>pren</rt></ruby>as <ruby>共<rt>komun</rt></ruby>ajn <ruby>课<rt>kurs</rt></ruby>ojn <ruby>何方<rt>kiel</rt></ruby> <ruby>群<rt>grup</rt></ruby>o. <ruby>间<rt>inter</rt></ruby> tiuj <ruby>课<rt>kurs</rt></ruby>oj <ruby>是<rt>est</rt></ruby>is la <ruby>学<rt>stud</rt></ruby>-<ruby>物<rt>objekt</rt></ruby>oj <ruby>今<rt>nun</rt></ruby><ruby>时间<rt>temp</rt></ruby>a <ruby>语<rt>esperant</rt></ruby>o-<ruby>文<rt>literatur</rt></ruby>o kaj <ruby>今<rt>nun</rt></ruby><ruby>时间<rt>temp</rt></ruby>aj <ruby>问题<rt>problem</rt></ruby>oj de la <ruby>语<rt>esperant</rt></ruby>o-<ruby>动<rt>mov</rt></ruby>ado. Tiun <ruby>末<rt>last</rt></ruby>an <ruby>导<rt>gvid</rt></ruby>is <ruby>标<rt>mark</rt></ruby> Fettes, <ruby>何u<rt>kiu</rt></ruby> <ruby>是<rt>est</rt></ruby>is <ruby>主<rt>prezid</rt></ruby>anto de <ruby>普<rt>universal</rt></ruby>a <ruby>语<rt>esperant</rt></ruby>o-<ruby>协会<rt>asoci</rt></ruby>o (UEA) de 2013 <ruby>至<rt>gxis</rt></ruby> 2019. <ruby>上<rt>sur</rt></ruby><ruby>基<rt>baz</rt></ruby>e de <ruby>自<rt>si</rt></ruby>aj <ruby>私<rt>propr</rt></ruby>aj <ruby>经<rt>spert</rt></ruby>oj, li <ruby>导<rt>gvid</rt></ruby>is la <ruby>学生<rt>student</rt></ruby>ojn en <ruby>兴趣<rt>interes</rt></ruby>aj <ruby>讨论<rt>diskut</rt></ruby>oj.\\n\\nUAM <ruby>高<rt>alt</rt></ruby>e <ruby>评<rt>taks</rt></ruby>as, ke la Interlingvistikaj <ruby>学<rt>stud</rt></ruby>oj <ruby>贡献<rt>kontribu</rt></ruby>as al la <ruby>间<rt>inter</rt></ruby><ruby>民<rt>naci</rt></ruby>igo de la <ruby>大学<rt>universitat</rt></ruby>o. <ruby>有效<rt>efektiv</rt></ruby>e, <ruby>外<rt>krom</rt></ruby> el Pol<ruby>土<rt>land</rt></ruby>o, la <ruby>学生<rt>student</rt></ruby>oj de<ruby>来<rt>ven</rt></ruby>as de 11 <ruby>土<rt>land</rt></ruby>oj: Brazilo, Britio, Cxehxio, Cxinio, Francio, Indonezio, Italio, Koreio, Kroatio, Rumanio, Vjetnamio. \\n\\nEn tiu cxi <ruby>年<rt>jar</rt></ruby>o la <ruby>学生<rt>student</rt></ruby>oj <ruby>作品<rt>verk</rt></ruby>as <ruby>再<rt>ankaux</rt></ruby> diplom-<ruby>工<rt>labor</rt></ruby><ruby>物<rt>ajx</rt></ruby>on. <ruby>自<rt>si</rt></ruby>ajn <ruby>结果<rt>rezult</rt></ruby>ojn <ruby>们<rt>ili</rt></ruby> <ruby>展示<rt>prezent</rt></ruby>os en <ruby>九月<rt>septembr</rt></ruby>o 2024. La <ruby>最<rt>plej</rt></ruby> <ruby>好<rt>bon</rt></ruby>aj <ruby>工<rt>labor</rt></ruby><ruby>物<rt>ajx</rt></ruby>oj <ruby>是<rt>est</rt></ruby>as <ruby>规<rt>regul</rt></ruby>e <ruby>外<rt>el</rt></ruby><ruby>赠<rt>don</rt></ruby>ataj, <ruby>例<rt>ekzempl</rt></ruby>e en la lingvistika <ruby>刊<rt>revu</rt></ruby>o Inv<ruby>是<rt>est</rt></ruby><ruby>使<rt>ig</rt></ruby>ationes <ruby>语<rt>lingv</rt></ruby><ruby>家<rt>ist</rt></ruby>icae de UAM.\\n\\n<ruby>后<rt>post</rt></ruby> 25 <ruby>年<rt>jar</rt></ruby>oj, tiu cxi <ruby>是<rt>est</rt></ruby>as la <ruby>末<rt>last</rt></ruby>a tri-<ruby>年<rt>jar</rt></ruby>a <ruby>时期<rt>period</rt></ruby>o de la <ruby>学<rt>stud</rt></ruby>oj. <ruby>如<rt>kvazaux</rt></ruby> por <ruby>庆<rt>fest</rt></ruby>i la 25-<ruby>年<rt>jar</rt></ruby><ruby>成<rt>igx</rt></ruby>on, en la <ruby>通过<rt>pas</rt></ruby>inta <ruby>学<rt>stud</rt></ruby><ruby>年<rt>jar</rt></ruby>o <ruby>开始<rt>komenc</rt></ruby><ruby>成<rt>igx</rt></ruby>is la magistraj <ruby>学<rt>stud</rt></ruby>oj <ruby>关<rt>pri</rt></ruby> interlingvistiko, <ruby>框<rt>kadr</rt></ruby>e de la <ruby>学<rt>stud</rt></ruby><ruby>科<rt>fak</rt></ruby>o Lingvistiko kaj <ruby>通<rt>inform</rt></ruby>-<ruby>老板<rt>mastr</rt></ruby>umado. La <ruby>学<rt>stud</rt></ruby>oj <ruby>好<rt>bon</rt></ruby>e <ruby>发展<rt>progres</rt></ruby>as <ruby>共<rt>kun</rt></ruby> dek <ruby>学生<rt>student</rt></ruby>oj, el <ruby>何u<rt>kiu</rt></ruby>j sep <ruby>来<rt>ven</rt></ruby>as el la cxina <ruby>大学<rt>universitat</rt></ruby>o en Zaozhuang, <ruby>何e<rt>kie</rt></ruby> <ruby>功能<rt>funkci</rt></ruby>as <ruby>语<rt>esperant</rt></ruby>o-<ruby>科<rt>fak</rt></ruby>o.\\n\\nEn 2024 la <ruby>学生<rt>student</rt></ruby>oj de <ruby>双<rt>ambaux</rt></ruby> <ruby>程序<rt>program</rt></ruby>oj <ruby>结束<rt>fin</rt></ruby>os <ruby>自<rt>si</rt></ruby>ajn <ruby>学<rt>stud</rt></ruby>ojn, kaj <ruby>ti方<rt>tiel</rt></ruby> <ruby>结束<rt>fin</rt></ruby><ruby>成<rt>igx</rt></ruby>os la <ruby>史<rt>histori</rt></ruby>o de interlingvistiko en UAM. <ruby>何方<rt>kiel</rt></ruby> <ruby>末<rt>last</rt></ruby>a <ruby>尖<rt>pint</rt></ruby>a <ruby>事件<rt>event</rt></ruby>o <ruby>发生<rt>okaz</rt></ruby>os <ruby>工<rt>labor</rt></ruby>-sesio <ruby>关<rt>pri</rt></ruby> interlingvistiko <ruby>框<rt>kadr</rt></ruby>e de la <ruby>名<rt>fam</rt></ruby>a <ruby>间<rt>inter</rt></ruby><ruby>民<rt>naci</rt></ruby>a <ruby>语<rt>lingv</rt></ruby><ruby>家<rt>ist</rt></ruby>a <ruby>会<rt>kongres</rt></ruby>o, <ruby>何u<rt>kiu</rt></ruby> cxi-<ruby>年<rt>jar</rt></ruby>e <ruby>发生<rt>okaz</rt></ruby>os en <ruby>姿<rt>poz</rt></ruby>nano, en <ruby>九月<rt>septembr</rt></ruby>o 2024.\\n\\n<ruby>岩<rt>rok</rt></ruby><ruby>健<rt>san</rt></ruby>a <ruby>是<rt>est</rt></ruby>is <ruby>外<rt>el</rt></ruby><ruby>立<rt>star</rt></ruby>a <ruby>客<rt>gast</rt></ruby><ruby>使<rt>ig</rt></ruby>antino kaj iun <ruby>夜<rt>vesper</rt></ruby>on <ruby>她<rt>sxi</rt></ruby> <ruby>故事<rt>rakont</rt></ruby>is al mi la <ruby>跟<rt>sekv</rt></ruby>an <ruby>故事<rt>rakont</rt></ruby>on.  Samoso <ruby>是<rt>est</rt></ruby>is <ruby>王<rt>princ</rt></ruby>-<ruby>土<rt>land</rt></ruby>o <ruby>内<rt>intern</rt></ruby>e de la Turka <ruby>帝<rt>imperi</rt></ruby>o. <ruby>i时<rt>iam</rt></ruby> la sultano en Istanbulo <ruby>有<rt>hav</rt></ruby>is <ruby>强<rt>fort</rt></ruby>an <ruby>痛<rt>dolor</rt></ruby>on <ruby>处<rt>cxe</rt></ruby> la <ruby>耳<rt>orel</rt></ruby>oj, <ruby>何u<rt>kiu</rt></ruby>n la istanbulaj <ruby>治<rt>kurac</rt></ruby><ruby>家<rt>ist</rt></ruby>oj ne <ruby>成功<rt>sukces</rt></ruby>is <ruby>治<rt>kurac</rt></ruby>i. Iu <ruby>通<rt>inform</rt></ruby>is la sultanon, ke en la <ruby>王<rt>princ</rt></ruby><ruby>土<rt>land</rt></ruby>o Samoso <ruby>是<rt>est</rt></ruby>as <ruby>名<rt>fam</rt></ruby>a <ruby>治<rt>kurac</rt></ruby><ruby>家<rt>ist</rt></ruby>o. La sultano <ruby>送<rt>send</rt></ruby>is <ruby>船<rt>sxip</rt></ruby>on por al<ruby>来<rt>ven</rt></ruby>igi la <ruby>治<rt>kurac</rt></ruby><ruby>家<rt>ist</rt></ruby>on St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby>, <ruby>共<rt>kun</rt></ruby> la <ruby>警<rt>avert</rt></ruby>o, ke li <ruby>是<rt>est</rt></ruby>os <ruby>残<rt>kruel</rt></ruby>e <ruby>罚<rt>pun</rt></ruby>ita, se io <ruby>误<rt>mis</rt></ruby><ruby>功能<rt>funkci</rt></ruby>os. Sed <ruby>后<rt>post</rt></ruby> unu <ruby>周<rt>semajn</rt></ruby>o la <ruby>耳<rt>orel</rt></ruby>oj de la sultano ne <ruby>增<rt>plu</rt></ruby> <ruby>痛<rt>dolor</rt></ruby>is. <ruby>何方<rt>kiel</rt></ruby> en <ruby>寓言<rt>fabel</rt></ruby>oj la sultano <ruby>说<rt>dir</rt></ruby>is al St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby>: “<ruby>谢<rt>dank</rt></ruby>on <ruby>因<rt>pro</rt></ruby> la <ruby>治<rt>kurac</rt></ruby>ado. <ruby>求<rt>pet</rt></ruby>u ion ajn, kaj mi <ruby>赠<rt>don</rt></ruby>os tion al vi.” St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> <ruby>说<rt>dir</rt></ruby>is: “Mi <ruby>愿<rt>vol</rt></ruby>as, ke en la <ruby>王<rt>princ</rt></ruby><ruby>土<rt>land</rt></ruby>o Samoso <ruby>人<rt>oni</rt></ruby> <ruby>教<rt>instru</rt></ruby>u <ruby>语<rt>esperant</rt></ruby>on en <ruby>全u<rt>cxiu</rt></ruby>j <ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>oj.” La sultano <ruby>转<rt>turn</rt></ruby>is sin al iu <ruby>大臣<rt>ministr</rt></ruby>o kaj <ruby>说<rt>dir</rt></ruby>is: \"<ruby>ti方<rt>tiel</rt></ruby> <ruby>是<rt>est</rt></ruby>u!\"\\n\\nLa <ruby>大臣<rt>ministr</rt></ruby>o <ruby>备<rt>pret</rt></ruby><ruby>使<rt>ig</rt></ruby>is <ruby>信<rt>leter</rt></ruby>on de la sultano al la <ruby>王<rt>princ</rt></ruby>o de Samoso <ruby>共<rt>kun</rt></ruby> la <ruby>命令<rt>ordon</rt></ruby>o, ke <ruby>人<rt>oni</rt></ruby> <ruby>教<rt>instru</rt></ruby>u <ruby>语<rt>esperant</rt></ruby>on en la <ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>oj, kaj la <ruby>船<rt>sxip</rt></ruby>o <ruby>再<rt>re</rt></ruby><ruby>运<rt>port</rt></ruby>is St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> kaj la <ruby>信<rt>leter</rt></ruby>on al Samoso. La <ruby>王<rt>princ</rt></ruby>o ne <ruby>有<rt>hav</rt></ruby>is <ruby>选择<rt>elekt</rt></ruby>on: li <ruby>必<rt>nepr</rt></ruby>e <ruby>必<rt>dev</rt></ruby>is <ruby>命令<rt>ordon</rt></ruby>i al la <ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>oj (<ruby>ti时<rt>tiam</rt></ruby> <ruby>是<rt>est</rt></ruby>is <ruby>能<rt>ebl</rt></ruby>e du-tri en tiu <ruby>岛<rt>insul</rt></ruby>o) <ruby>教<rt>instru</rt></ruby>i <ruby>语<rt>esperant</rt></ruby>on. Sed la <ruby>祭司<rt>pastr</rt></ruby>oj kaj la <ruby>公<rt>publik</rt></ruby>o <ruby>是<rt>est</rt></ruby>is <ruby>极<rt>tre</rt></ruby> <ruby>非<rt>mal</rt></ruby><ruby>满<rt>kontent</rt></ruby>aj, <ruby>因<rt>cxar</rt></ruby> St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> <ruby>是<rt>est</rt></ruby>is <ruby>识<rt>kon</rt></ruby>ata <ruby>社<rt>social</rt></ruby><ruby>家<rt>ist</rt></ruby>o, kaj <ruby>人<rt>oni</rt></ruby> <ruby>考虑<rt>konsider</rt></ruby>is lin <ruby>非<rt>mal</rt></ruby><ruby>朋<rt>amik</rt></ruby>o de <ruby>神<rt>di</rt></ruby>o kaj de la krist<ruby>员<rt>an</rt></ruby>a <ruby>宗教<rt>religi</rt></ruby>o. La <ruby>祭司<rt>pastr</rt></ruby>oj <ruby>宣<rt>predik</rt></ruby>is al la <ruby>民<rt>popol</rt></ruby>o, ke <ruby>人<rt>oni</rt></ruby> <ruby>必<rt>dev</rt></ruby>as <ruby>非<rt>mal</rt></ruby><ruby>帮<rt>help</rt></ruby>i <ruby>ti方<rt>tiel</rt></ruby> <ruby>恐<rt>terur</rt></ruby>an <ruby>事<rt>afer</rt></ruby>on <ruby>因<rt>pro</rt></ruby> <ruby>罪<rt>kulp</rt></ruby>o de la <ruby>非<rt>mal</rt></ruby><ruby>强<rt>fort</rt></ruby>a <ruby>王<rt>princ</rt></ruby>o, <ruby>何u<rt>kiu</rt></ruby> <ruby>服<rt>obe</rt></ruby>is <ruby>命令<rt>ordon</rt></ruby>on de la islama <ruby>帝<rt>imperi</rt></ruby><ruby>长<rt>estr</rt></ruby>o <ruby>教<rt>instru</rt></ruby>i <ruby>语<rt>lingv</rt></ruby>on de la <ruby>魔<rt>diabl</rt></ruby>o. <ruby>们<rt>ili</rt></ruby> <ruby>ti方<rt>tiel</rt></ruby> <ruby>效<rt>efik</rt></ruby>e <ruby>宣<rt>predik</rt></ruby>is, ke la <ruby>民<rt>popol</rt></ruby>o <ruby>燃<rt>brul</rt></ruby><ruby>使<rt>ig</rt></ruby>is la <ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>ojn. <ruby>ti时<rt>tiam</rt></ruby> la <ruby>王<rt>princ</rt></ruby>o <ruby>止<rt>cxes</rt></ruby><ruby>使<rt>ig</rt></ruby>is la <ruby>教<rt>instru</rt></ruby>adon de <ruby>语<rt>esperant</rt></ruby>o, kaj St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> <ruby>自<rt>mem</rt></ruby> <ruby>隔<rt>trans</rt></ruby><ruby>住<rt>logx</rt></ruby><ruby>成<rt>igx</rt></ruby>is al Istanbulo kaj <ruby>后<rt>post</rt></ruby>e al Ateno. La <ruby>全<rt>tut</rt></ruby>a <ruby>事<rt>afer</rt></ruby>o <ruby>发生<rt>okaz</rt></ruby>is <ruby>约<rt>cxirkaux</rt></ruby> la <ruby>年<rt>jar</rt></ruby>o 1910.\\n\\nTiu <ruby>是<rt>est</rt></ruby>as la <ruby>故事<rt>rakont</rt></ruby>o, <ruby>何u<rt>kiu</rt></ruby>n mi <ruby>听<rt>auxd</rt></ruby>is de <ruby>岩<rt>rok</rt></ruby><ruby>健<rt>san</rt></ruby>a St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby>, (aux \"s-ino Manusu\" <ruby>依<rt>laux</rt></ruby> la <ruby>名<rt>nom</rt></ruby>o de <ruby>她<rt>sxi</rt></ruby>a <ruby>夫<rt>edz</rt></ruby>o). <ruby>生<rt>nask</rt></ruby><ruby>成<rt>igx</rt></ruby>inte en 1900, <ruby>她<rt>sxi</rt></ruby> <ruby>后<rt>post</rt></ruby>e <ruby>死<rt>mort</rt></ruby>is en 1981.\\n\\nSed <ruby>依<rt>laux</rt></ruby> <ruby>探<rt>esplor</rt></ruby>oj el <ruby>异<rt>ali</rt></ruby>aj <ruby>源<rt>font</rt></ruby>oj la <ruby>实<rt>real</rt></ruby>o <ruby>是<rt>est</rt></ruby>is <ruby>i量<rt>iom</rt></ruby> <ruby>非<rt>mal</rt></ruby><ruby>同<rt>sam</rt></ruby>a kaj <ruby>非<rt>mal</rt></ruby><ruby>于<rt>pli</rt></ruby> <ruby>浪漫<rt>romantik</rt></ruby>a. St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> <ruby>是<rt>est</rt></ruby>is <ruby>重<rt>grav</rt></ruby>a <ruby>治<rt>kurac</rt></ruby><ruby>家<rt>ist</rt></ruby>o en Samoso kaj <ruby>活<rt>aktiv</rt></ruby>a <ruby>语<rt>esperant</rt></ruby><ruby>家<rt>ist</rt></ruby>o. Li <ruby>创<rt>fond</rt></ruby>is <ruby>语<rt>esperant</rt></ruby>o-<ruby>群<rt>grup</rt></ruby>on en Samoso <ruby>约<rt>cxirkaux</rt></ruby> la <ruby>年<rt>jar</rt></ruby>o 1905, <ruby>外<rt>el</rt></ruby><ruby>赠<rt>don</rt></ruby>is <ruby>报纸<rt>gazet</rt></ruby>on, kaj <ruby>说服<rt>konvink</rt></ruby>is la <ruby>王<rt>princ</rt></ruby>on <ruby>做<rt>far</rt></ruby>i dekreton <ruby>关<rt>pri</rt></ruby> <ruby>学<rt>lern</rt></ruby>ado de <ruby>语<rt>esperant</rt></ruby>o en la <ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>oj de Samoso. La <ruby>学<rt>lern</rt></ruby>ado de <ruby>语<rt>esperant</rt></ruby>o <ruby>止<rt>cxes</rt></ruby>is <ruby>何am<rt>kiam</rt></ruby> la <ruby>王<rt>princ</rt></ruby>o <ruby>是<rt>est</rt></ruby>is <ruby>死<rt>mort</rt></ruby><ruby>使<rt>ig</rt></ruby>ita kaj St<ruby>爱<rt>am</rt></ruby>atia<ruby>散<rt>dis</rt></ruby> <ruby>是<rt>est</rt></ruby>is <ruby>放逐<rt>ekzil</rt></ruby>ita al Istanbulo <ruby>因<rt>pro</rt></ruby> <ruby>不<rt>ne</rt></ruby><ruby>明<rt>klar</rt></ruby>aj <ruby>政<rt>politik</rt></ruby>aj <ruby>何因<rt>kial</rt></ruby>oj. \\n\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=replace_esperanto_chars(text,esperanto_to_x)\n",
    "safe_replace(text,replacements3)\n",
    "# parallel_process(text*1,1, replacements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c32cc1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ruby>世界<rt>mond</rt></ruby>on <ruby>世界<rt>mond</rt></ruby>o <ruby>酒<rt>vin</rt></ruby>o <ruby>酒<rt>vin</rt></ruby>on  <ruby>酒<rt>vin</rt></ruby>ojn vin <ruby>酒<rt>vin</rt></ruby>o <ruby>新<rt>nov</rt></ruby>a '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# safe_replace(\"Membroj havas  Mi farigxis multaj taskoj  Mi amas vin multe necese movado sin senegal nova \",replacements3)\n",
    "safe_replace(\"mondon mondo vino vinon  vinojn vin vino nova \",replacements3)\n",
    "##sin sindoni sindonemo dion Dion amon　amon dio dia Senegala senegal senegala senegalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a167267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_strings_in_text(text):\n",
    "    # 正規表現パターンを定義\n",
    "    pattern = re.compile(r'%%(.{1,20}?)%%')\n",
    "    matches = []\n",
    "    used_indices = set()\n",
    "\n",
    "    # 正規表現のマッチを見つける\n",
    "    for match in pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "        # 重複する%%を避けるためにインデックスをチェック\n",
    "        if start not in used_indices and end-2 not in used_indices:  # end-2 because of double %%\n",
    "            matches.append(match.group(1))\n",
    "            # インデックスを使用済みセットに追加\n",
    "            used_indices.update(range(start, end))\n",
    "    return matches\n",
    "def load_placeholders(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        placeholders = [line.strip() for line in file if line.strip()]\n",
    "    return placeholders\n",
    "def create_replacements(text, placeholders):\n",
    "    # テキストから%%で囲まれた部分を抽出\n",
    "    matches = find_strings_in_text(text)\n",
    "    replacements_list_for_intact_parts = []\n",
    "    # プレースホルダーとマッチを対応させる\n",
    "    for i, match in enumerate(matches):\n",
    "        if i < len(placeholders):\n",
    "            replacements_list_for_intact_parts.append([f\"%%{match}%%\", placeholders[i]])\n",
    "        else:\n",
    "            break  # プレースホルダーが足りなくなった場合は終了\n",
    "    return replacements_list_for_intact_parts\n",
    "# 使用例\n",
    "text = text = \"\"\"This is a test %%str\n",
    "ing1%% in a text with %%another\n",
    "_string2%% a\n",
    "nd %%sho\n",
    "rt%% but not %%this\n",
    "_one%% u%%s\n",
    "e%%d again.   kshd%%f%%s%%dsd%%\n",
    "%%sijajcipsj%%jocokcowkcoskcpokosakxokcxokcokcwockwp%%cisjc%%\n",
    "\"\"\"\n",
    "# プレースホルダーファイルから読み込む\n",
    "placeholders = load_placeholders('No.1000_9999.txt')\n",
    "# リストを作成\n",
    "replacements_list_for_intact_parts = create_replacements(text, placeholders)\n",
    "sorted_replacements_list_for_intact_parts = sorted(replacements_list_for_intact_parts, key=lambda x: len(x[0]), reverse=True)\n",
    "# 結果を表示\n",
    "# for item in sorted_replacements_list_for_intact_parts:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bdd131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('例文2.txt','r') as g:\n",
    "    ll=g.read()\n",
    "text2=replace_esperanto_chars(ll,esperanto_to_x)\n",
    "\n",
    "replacements_list_for_intact_parts = create_replacements(text2, placeholders)\n",
    "sorted_replacements_list_for_intact_parts = sorted(replacements_list_for_intact_parts, key=lambda x: len(x[0]), reverse=True)\n",
    "for original, place_holder_ in sorted_replacements_list_for_intact_parts:\n",
    "    text2 = text2.replace(original, place_holder_)\n",
    "\n",
    "text3=safe_replace(text2, replacements2)\n",
    "\n",
    "for original, place_holder_ in sorted_replacements_list_for_intact_parts:\n",
    "    text3 = text3.replace(place_holder_, original.replace(\"%%\",\"\"))\n",
    "\n",
    "# text3=parallel_process(text2*1,1, replacements2)\n",
    "with open('出力2.html','w', encoding='utf-8') as h:\n",
    "    h.write(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7713af7",
   "metadata": {},
   "source": [
    "以下是附录(以下はおまけ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c92b78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイルのエンコーディング問題　　オリジナルの\"pejvo.txt\"を\"世界语全部单词_大约44000个(原pejvo.txt).txt\"に改名する際、encodingも'SHIFT_JIS'から'utf-8'に変更した。\n",
    "\n",
    "# import chardet\n",
    "# with open('pejvo.txt', 'rb') as f:\n",
    "#     result = chardet.detect(f.read())\n",
    "# print(result['encoding'])\n",
    "# # 'SHIFT_JIS'エンコーディングでファイルを読み込む\n",
    "# with open('pejvo.txt', 'r', encoding='SHIFT_JIS') as f:\n",
    "#     content = f.read()\n",
    "# # UTF-8エンコーディングで内容をファイルに書き戻す\n",
    "# with open('世界语全部单词_大约44000个(原pejvo.txt).txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "680551ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##収録されているエスペラントの単語の語尾の確認(特に名詞、形容詞、副詞、動詞はどのような形式(接尾辞)で収録されているか)\n",
    "not_aiueojn_list=[]\n",
    "# テキストファイルを開きます。\n",
    "with open(\"tmp.txt\", 'r', encoding='utf-8') as file:\n",
    "    # 各行をループします。\n",
    "    for line in file:\n",
    "        # \":\"が出てくるまでの部分を取り出します。\n",
    "        word = line.split(\":\")[0]\n",
    "        # さらに取り出した部分を\"-, \"で分けます。\n",
    "        parts = re.split('-| ', word)\n",
    "        # 各部分をループします。\n",
    "        for part in parts:\n",
    "            if not part.endswith(('a', 'i', 'u', 'e', 'o','oj','aj','/on','/e/n','/an','on!','/e!','ojn','/an!','/e/n!','/o!','/u!')):\n",
    "                not_aiueojn_list.append(part)\n",
    "with open(\"irregular_suffix.txt\",\"w\",encoding='utf-8') as g:\n",
    "    for hh in not_aiueojn_list:\n",
    "        g.write(hh+\"\\n\")\n",
    "##収録されているエスペラントの単語の語尾の確認(特に名詞、形容詞、副詞、動詞はどのような形式(接尾辞)で収録されているか)\n",
    "aiueojn_list=[]\n",
    "# テキストファイルを開きます。\n",
    "with open(\"tmp.txt\", 'r', encoding='utf-8') as file:\n",
    "    # 各行をループします。\n",
    "    for line in file:\n",
    "        # \":\"が出てくるまでの部分を取り出します。\n",
    "        word = line.split(\":\")[0]\n",
    "        # 取り出した部分を\"-, \"で分けます。\n",
    "        parts = re.split('-| ', word)\n",
    "        # 各部分をループします。\n",
    "        for part in parts:\n",
    "            if part.endswith(('/on','/e/n','/an','/an!','/e/n!','/o!','/u!')):\n",
    "                aiueojn_list.append(part)\n",
    "\n",
    "with open(\"regular_suffix.txt\",\"w\",encoding='utf-8') as g:\n",
    "    for hh in aiueojn_list:\n",
    "        g.write(hh+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ada2b088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacements2にのみ存在する要素: set()\n",
      "replacements3にのみ存在する要素: set()\n",
      "リストは同一です。\n"
     ]
    }
   ],
   "source": [
    "#リストreplacements2とリストreplacements3が同一であることの確認\n",
    "# 集合に変換\n",
    "set_replacements2 = set(replacements2)\n",
    "set_replacements3 = set(replacements3)\n",
    "# 差分を取得\n",
    "diff_replacements2 = set_replacements2 - set_replacements3\n",
    "diff_replacements3 = set_replacements3 - set_replacements2\n",
    "# 結果を表示\n",
    "print(\"replacements2にのみ存在する要素:\", diff_replacements2)\n",
    "print(\"replacements3にのみ存在する要素:\", diff_replacements3)\n",
    "# 違いがあるかどうかを調べる\n",
    "if diff_replacements2 or diff_replacements3:\n",
    "    print(\"リスト同士に違いが存在します。\")\n",
    "else:\n",
    "    print(\"リストは同一です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###テキストファイルの行をエスペラントアルファベット順に並び替えたり、重複行を消すためのコード。おそらく使うことは無いだろう。\n",
    "def esperanto_order(char):\n",
    "    # エスペラントのアルファベット順\n",
    "    order = \"abcĉdefgĝhĥijĵklmnoprsŝtuŭvz\"\n",
    "    # 文字がエスペラントのアルファベット順でどこにあるかを返す。存在しない場合は -1 を返す。\n",
    "    return order.index(char) if char in order else -1\n",
    "\n",
    "def sort_key(line):\n",
    "    # エスペラントの特殊文字を含む場合の並び替えキーを生成し、改行文字を除去\n",
    "    return [esperanto_order(c) for c in line.strip()]\n",
    "\n",
    "# 入力ファイルを読み込み\n",
    "with open('检查世界语所有单词的结尾是否被正确切除(result)__.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# リストをエスペラントのアルファベット順にソート\n",
    "lines.sort(key=sort_key)\n",
    "\n",
    "# ソートされた内容を新しいファイルに書き出す\n",
    "with open('sorted_output2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(lines)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 入力ファイルを読み込み\n",
    "with open('sorted_output2.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# OrderedDictを使用して重複を削除し、挿入順序を保持\n",
    "unique_lines = list(OrderedDict.fromkeys(line.strip() for line in lines))\n",
    "\n",
    "# 重複のない内容を新しいファイルに書き出す\n",
    "with open('unique_sorted_output.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(line + '\\n' for line in unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aab78f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def contains_digit(s):#対象の文字列sに数字となりうる文字列(数字)が含まれるかどうかを確認する関数\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "result=[]\n",
    "# \"tmp.txt\"(\"世界语全部单词_大约44100个(原pejvo.txt).txt\"を小文字、X形式に変換したもの)を開く。\n",
    "with open(\"tmp.txt\", 'r', encoding='utf-8') as file:\n",
    "    # \"tmp.txt\"の各行をループ。\n",
    "    for line in file:\n",
    "        # ':'が出てくるまでの部分を取り出す。\n",
    "        word = line.split(\":\")[0]\n",
    "        # さらに取り出した部分を'-'、' '、','で分ける。\",\"もごくまれに存在する('tial')\n",
    "        parts = re.split('-| |,', word)\n",
    "        # 各部分をループし、単語の語尾の形式によって品詞分類しながら、その語尾をカットする。\n",
    "        for numb in range(len(parts)):\n",
    "            if numb>=1:\n",
    "                part=parts[numb]\n",
    "                if not (contains_digit(part) or len(part)<2):\n",
    "                    if \"/\" in part:\n",
    "                        if part.endswith(('/o','/on','oj','/o!','ojn','on!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('名詞')\n",
    "                        elif part.endswith(('/a','/aj','/an','/an!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('形容詞')\n",
    "                        elif part.endswith(('/e','/e!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('副詞')\n",
    "                        elif part.endswith(('/e/n','/e/n!')):##'/e/n'は後で気をつける\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-2])]\n",
    "                            AA.append('副詞')    \n",
    "                        elif part.endswith(('/i','/u','/u!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('動詞')\n",
    "                        elif part.endswith(('/n')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('n語')            \n",
    "                    else:\n",
    "                        AA=[part,\"無詞\"]\n",
    "                    result.append(AA)\n",
    "##すべての単語の語尾が正しくカットされているかどうかチェックする。\n",
    "with open(\"2列目以降.txt\",\"w\",encoding='utf-8') as g:\n",
    "    for hh in result:\n",
    "        if len(hh)==2:##不要な条件と思われる。\n",
    "            g.write(hh[0]+','+hh[1]+\"\\n\")\n",
    "            # g.write(replace_esperanto_chars(hh[0],x_to_jijofu)+\"##\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a802b6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tra/met/as tra/met/is tra/met/os tra/met/us tra/met/at tra/met/it tra/met/ot tra/met/ad tra/met/igx tra/met/ig tra/met/ant tra/met/int tra/met/ont tra/met/u  tra/met/u!\n"
     ]
    }
   ],
   "source": [
    "verb_suffixes = ['as', 'is', 'os', 'us', 'at', 'it', 'ot', 'ad', 'igx', 'ig', 'ant', 'int', 'ont','u ','u!']\n",
    "base_string = \"tra/met/\"\n",
    "# base_string = \"dat/um/\"\n",
    "# リスト内包表記を使用して、基本文字列に各接尾語を追加\n",
    "result_strings = [base_string + suffix for suffix in verb_suffixes]\n",
    "\n",
    "# 結果をスペースで区切って表示\n",
    "result = \" \".join(result_strings)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元のエンコーディングでファイルを開く\n",
    "with open(\"世界语全部单词_大约44100个(原pejvo.txt)_original2024620.txt\", \"r\", encoding=\"cp932\") as file:\n",
    "    text = file.read()\n",
    "# 'utf-8' で新しいファイルに保存する(1.7MBが2.1MBになる。)\n",
    "with open(\"世界语全部单词_大约44100个(原pejvo.txt)_original2024620_utf8.txt\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "    new_file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix_2l={'bo':'bo', 'ek':'ek', 'ge':'ge', 're':'re'}\n",
    "# prefix_3l={'cxef':'cxef', 'dis':'dis', 'eks':'eks', 'for':'for', 'mal':'mal', 'post':'post'}\n",
    "# suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul'}\n",
    "# suffix_3l={'acx':'acx',\t'ajx':'ajx'\t, 'ebl':'ebl',\t'end':'end','estr':'estr','igx':'igx','ind':'ind','ing':'ing','ism':'ism','ist':'ist','obl':'obl','ant':'ant','int':'int','ont':'ont'}\n",
    "# prefix_2l_2={'bo': 'bo', 'ek': 'ek', 'ge': 'ge', 're': 're'}\n",
    "# prefix_3l_2={'cxef': '首(cxef)','dis': '散(dis)','eks': '前(eks)','for': '离(for)','mal': '非(mal)','post': '后(post)'}\n",
    "# suffix_2l_2={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul'}\n",
    "# suffix_3l_2={'acx': '劣(acx)','ajx': '物(ajx)','ebl': '能(ebl)','end': '必(end)','estr': '长(estr)','igx': '成(igx)','ind': '价(ind)','ing': '壳(ing)','ism': '义(ism)','ist': '家(ist)','obl': '倍(obl)','ant': 'ant','int': 'int','ont': 'ont'}\n",
    "# noun_prefix_2l={'bo':'bo', 'ek':'ek', 'ge':'ge', 're':'re', 'cxef':'cxef', 'dis':'dis', 'eks':'eks', 'for':'for', 'mal':'mal', 'post':'post'}\n",
    "# noun_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul', \n",
    "#            'acx':'acx',\t'ajx':'ajx'\t, 'ebl':'ebl',\t'end':'end','estr':'estr','igx':'igx','ind':'ind','ing':'ing','ism':'ism','ist':'ist','obl':'obl','ant':'ant','int':'int','ont':'ont'}\n",
    "# adj_prefix_2l={'bo':'bo', 'ek':'ek', 'ge':'ge', 're':'re', 'cxef':'cxef', 'dis':'dis', 'eks':'eks', 'for':'for', 'mal':'mal', 'post':'post'}\n",
    "# adj_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul', \n",
    "#            'acx':'acx',\t'ajx':'ajx'\t, 'ebl':'ebl',\t'end':'end','estr':'estr','igx':'igx','ind':'ind','ing':'ing','ism':'ism','ist':'ist','obl':'obl','ant':'ant','int':'int','ont':'ont'}\n",
    "# adv_prefix_2l={'bo':'bo', 'ek':'ek', 'ge':'ge', 're':'re', 'cxef':'cxef', 'dis':'dis', 'eks':'eks', 'for':'for', 'mal':'mal', 'post':'post'}\n",
    "# adv_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul', \n",
    "#            'acx':'acx',\t'ajx':'ajx'\t, 'ebl':'ebl',\t'end':'end','estr':'estr','igx':'igx','ind':'ind','ing':'ing','ism':'ism','ist':'ist','obl':'obl','ant':'ant','int':'int','ont':'ont'}\n",
    "# verb_prefix_2l={'bo':'bo', 'ek':'ek', 'ge':'ge', 're':'re', 'cxef':'cxef', 'dis':'dis', 'eks':'eks', 'for':'for', 'mal':'mal', 'post':'post'}\n",
    "# verb_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us', 'um':'um','at':'at','it':'it','ot':'ot', 'ad':'ad','an':'an','ar':'ar','ec':'ec', 'eg':'eg',\t'ej':'ej', 'em':'em', 'er':'er', 'et':'et',\t'id':'id', 'ig':'ig', 'il':'il', 'in':'in', 'on':'on', 'op':'op', 'uj':'uj', 'ul':'ul', \n",
    "#            'acx':'acx',\t'ajx':'ajx'\t, 'ebl':'ebl',\t'end':'end','estr':'estr','igx':'igx','ind':'ind','ing':'ing','ism':'ism','ist':'ist','obl':'obl','ant':'ant','int':'int','ont':'ont'}\n",
    "# noun_prefix_2l_2={}\n",
    "# for d1,d2 in noun_prefix_2l.items():\n",
    "#     noun_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# noun_suffix_2l_2={}\n",
    "# for d1,d2 in noun_suffix_2l.items():\n",
    "#     noun_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# adj_prefix_2l_2={}\n",
    "# for d1,d2 in adj_prefix_2l.items():\n",
    "#     adj_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# adj_suffix_2l_2={}\n",
    "# for d1,d2 in adj_suffix_2l.items():\n",
    "#     adj_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# adv_prefix_2l_2={}\n",
    "# for d1,d2 in adv_prefix_2l.items():\n",
    "#     adv_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# adv_suffix_2l_2={}\n",
    "# for d1,d2 in adv_suffix_2l.items():\n",
    "#     adv_suffix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# verb_prefix_2l_2={}\n",
    "# for d1,d2 in verb_prefix_2l.items():\n",
    "#     verb_prefix_2l_2[d1]=safe_replace(d2, replacements)\n",
    "# verb_suffix_2l_2={}\n",
    "# for d1,d2 in verb_suffix_2l.items():\n",
    "#     verb_suffix_2l_2[d1]=safe_replace(d2, replacements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)の要素を全て結合して、一つの文字列にしてから漢字置換を実施すれば\n",
    "#高速化できるのではないかと試してみたが、大して速くならなかった。\n",
    "# combined = \"#%\".join([entry[0] for entry in result])\n",
    "# (safe_replace(combined, replacements)).split(\"#%\")[:20]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スラッシュを取り除いたキーでデータを整理するための辞書\n",
    "normalized_keys = {}\n",
    "# 各キーからスラッシュを取り除き、既存のキーとして整理\n",
    "for old, value in SS.items():\n",
    "    # スラッシュを取り除く\n",
    "    normalized_key = old.replace('/', '')\n",
    "\n",
    "    # 辞書に追加\n",
    "    if normalized_key not in normalized_keys:\n",
    "        normalized_keys[normalized_key] = []\n",
    "    normalized_keys[normalized_key].append((old, value))\n",
    "\n",
    "# 抜き出すためのファイル出力  修正がうまく言ったので、ファイルサイズは0になるはず。\n",
    "with open(\"SS_chofuku.txt\", 'w', encoding='utf-8') as file:\n",
    "    for key, entries in normalized_keys.items():\n",
    "        # 同じ語根を持つ要素が複数ある場合のみファイルに書き込む\n",
    "        if len(entries) > 1:\n",
    "            for old, value in entries:\n",
    "                file.write(f'{old},{value[0]},{value[1]}\\n')\n",
    "\n",
    "# 各語根ごとに最長のキーを保持する辞書\n",
    "max_length_keys = {}\n",
    "\n",
    "# スラッシュを取り除いた語根をキーとして、最長のキーと値を保存\n",
    "for old, value in SS.items():\n",
    "    # スラッシュを取り除く\n",
    "    normalized_key = old.replace('/', '')\n",
    "    # 辞書にこの語根が存在するか、存在する場合は現在のキーと比較\n",
    "    if normalized_key not in max_length_keys or len(max_length_keys[normalized_key][0]) < len(old):\n",
    "        max_length_keys[normalized_key] = (old, value)\n",
    "\n",
    "# 最終的な辞書を作成\n",
    "SS_ = {k: v for _, (k, v) in max_length_keys.items()}\n",
    "\n",
    "# 結果を出力\n",
    "# print(SS_)\n",
    "###品詞によって分けることは可能か？20240616\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350bdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32045, 32045)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"SS.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  SS.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]}\\n')\n",
    "with open(\"SS_.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  SS_.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]}\\n')\n",
    "len(SS),len(SS_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20029a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"名詞\" in j[1]:\n",
    "#     for k in [\"bo\",\"ek\",\"eks\",\"ge\",\"mal\",\"mis\",\"pra\",\"re\",\"sam\"]: \n",
    "#     for k in [\"acx\",\"ad\",\"ajx\",\"an\",\"ar\",\"ebl\",\"ec\",\"eg\",\"ej\",\"em\",\"er\",\"et\",\"id\",\"il\",\"in\",\"ism\",\"ist\",\"uj\",\"ul\",\"um\",\"oj\",\"sen\"]:##\"obl\",\"on\",\"op\",\n",
    "#     for k in [\"o\"]:          \n",
    "# if \"形容詞\" in j[1]:\n",
    "#     for k in [\"ne\",\"pra\",\"ek\",\"mal\",\"mis\",\"sen\",]:\n",
    "#     for k in [\"acx\",\"et\",\"eg\",\"em\",\"ind\",\"ebl\",\"er\",\"aj\",\"iv\"]:\n",
    "#     for k in [\"a\"]:\n",
    "# if \"副詞\" in j[1]:\n",
    "#     for k in [\"mal\",\"ne\",\"mis\",\"re\",\"retro\",\"sen\"]:\n",
    "#     for k in [\"acx\",\"et\",\"eg\",\"ere\",\"om\",\"op\"]:\n",
    "#     for k in [\"e\"]: \n",
    "# if \"動詞\" in j[1]:\n",
    "#     for k in [\"ek\",\"el\",\"en\",\"re\",\"mal\",\"dis\",\"mis\",\"retro\",\"for\"]:\n",
    "#     for k in [\"ad\",\"acx\",\"et\",\"eg\",\"em\",\"ind\",\"ebl\",\"end\",\"er\",\"igx\",\"ig\",\"as\",\"os\",\"is\",\"ant\",\"int\",\"ont\",\"at\",\"it\",\"ot\",\"us\"]:\n",
    "#     for k in [\"u,i\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if part.endswith(('/o','/on','oj','/o!','ojn','on!')):\n",
    "#                             AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "#                             AA.append('名詞')\n",
    "#                         elif part.endswith(('/a','/aj','/an','/an!')):\n",
    "#                             AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "#                             AA.append('形容詞')\n",
    "#                         elif part.endswith(('/e','/e!')):\n",
    "#                             AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "#                             AA.append('副詞')\n",
    "#                         elif part.endswith(('/e/n','/e/n!')):##'/e/n'は後で気をつける\n",
    "#                             AA=[\"/\".join(part.split(\"/\")[:-2])]\n",
    "#                             AA.append('副詞')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f42a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#上の作業で作成した辞書型リスト(SS)の最初から20個分を表示\n",
    "# for key, value in dict(list(QQ.items())[:10]).items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_placeholder_text(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for i in range(1000, 10000):\n",
    "            placeholder = f\"&%{i}&%\"\n",
    "            file.write(placeholder + '\\n')  # 各プレースホルダーを一行ずつ書き込みます\n",
    "\n",
    "# 使用例: プレースホルダーを含むテキストファイルを生成\n",
    "filename = 'No.1000_9999.txt'\n",
    "generate_placeholder_text(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
